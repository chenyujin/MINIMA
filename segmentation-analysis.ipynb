{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a9c933",
   "metadata": {},
   "source": [
    "1. preprocessing in FIJI: cut out single colonies, increase contrast, substrate BG (ball 50), threshold and create mask, mask apply on BG-sub (nbg) [not used]\n",
    "2. Ilastik label pixels, segment. Manual correction with Fiji\n",
    "3. Contour analysis with cv2\n",
    "4. TrackMate, map contours and tracks between TrackMate and cv2\n",
    "\n",
    "Some old code on cell-tracking directly from cv2 contour, skeleton analysis, \n",
    "\n",
    "Note: \n",
    "* Colony-level analysis (area growth rate, MTMT area, density) done with \"Area-analysis\"\n",
    "* Plot check on video moved to \"RatePlot\" -cellID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5dd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the essential packages \n",
    "import numpy as np   # for numerics\n",
    "import glob   #for parsing directories and files\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  #for plotting\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import cm\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df368c7a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "data structure: exppath (by type and date), replica (by index)\n",
    "\"\"\"\n",
    "# exppath = 'ForPub/300-LBLMagar/'  #   240307 231006-60x\n",
    "# rplcpathList = [f.path for f in os.scandir(exppath) if f.is_dir()].sorted\n",
    "\n",
    "# rplcpath ='ForPub/600-LBLMagar/00798-2'\n",
    "# rplcpath ='ForPub/150-LBLMagar/00788'\n",
    "rplcpath ='ForPub/075-LBLMagar/00814'\n",
    "head, rplcIdx = os.path.split(rplcpath)\n",
    "\n",
    "# nbgName = rplcIdx + '-BG.tif'\n",
    "# segName = rplcIdx + '-BG_Simple Segmentation.tiff' \n",
    "# dataDicName = rplcIdx + '-dataDic.pkl'\n",
    "# divName = rplcIdx + '-Div.txt'\n",
    "# trajDicName = rplcIdx + '-trajDic.pkl'\n",
    "# print(os.path.join(rplcpath, trajDicName))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43534118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "20250303 rewritten by ChatGPT\n",
    "cv2 contour analysis, result given in DataFrame\n",
    "20250309 revise, \"bounding box\" + \"fit elipse\" \n",
    "Note: \"skeleton analysis\" is done by another library, if needed, may need to match contour again\n",
    "\n",
    "---Input---\n",
    "Ilastik segmented, binary image series. \n",
    "Cells must be separated (# check by plot contour and TrackMate).\n",
    "\n",
    "---Output---\n",
    "Pandas DataFrame: \n",
    "[\"frame\", \"spot_ID\", \"x\", \"y\", \"area\", \"solidity\", \"rect_length\", \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\"]\n",
    "_Note:_ when read in TrackMate spots.csv, usecols =[\"ID\", \"POSITION_X\",\"POSITION_Y\", \"FRAME\", \"ELLIPSE_MAJOR\",\"ELLIPSE_THETA\", \"SOLIDITY\"] \n",
    "\"\"\"\n",
    "rplcpath ='ForPub/150-LBLMagar/250717-150-2-ana/00955'\n",
    "# rplcpath ='ForPub/150-LBLMagar/240228-150-1-ana/00321-2'\n",
    "head, rplcIdx = os.path.split(rplcpath)\n",
    "tmpath = rplcpath + '/trackmate'\n",
    "\n",
    "segName = rplcIdx + '-BG_Simple Segmentation.tiff' \n",
    "fpath = os.path.join(rplcpath, segName)\n",
    "dataFName = rplcIdx + '-spots_cv2.csv'\n",
    "dataFpath = os.path.join(tmpath, dataFName)\n",
    "\n",
    "img_segs = []\n",
    "ret, img_segs = cv2.imreadmulti(mats=img_segs, filename=fpath, \n",
    "#                                 start=0, count=81,\n",
    "                                flags=cv2.IMREAD_GRAYSCALE)\n",
    "data_list = [] \n",
    "for i in range(len(img_segs)):\n",
    "    img_seg = img_segs[i]\n",
    "    contours, hierarchy = cv2.findContours(img_seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    " \n",
    "    for j, ct in enumerate(contours):\n",
    "        area = cv2.contourArea(ct)\n",
    "        if area > 50:  # Size range for cells\n",
    "            M = cv2.moments(ct)\n",
    "            if M['m00'] == 0:\n",
    "                continue  # Avoid division by zero           \n",
    "            # Compute centroid\n",
    "            cntx = M['m10'] / M['m00']\n",
    "            cnty = M['m01'] / M['m00']\n",
    "            \n",
    "            # Get minimum area bounding rectangle\n",
    "            rect = cv2.minAreaRect(ct)\n",
    "            width, height = rect[1]  # (w, h) of the rotated bounding box\n",
    "            angle_rect = rect[2]           \n",
    "            # Ensure width is always the smaller side and height is the longer side\n",
    "            length_rect = max(width, height)\n",
    "            aspect_ratio = length_rect / min(width, height)\n",
    "\n",
    "            ellipse = cv2.fitEllipse(ct)  # (center_x, center_y), (major_axis, minor_axis), angle\n",
    "            major_axis, minor_axis = ellipse[1]  # note: from test, it's (minor, major)\n",
    "            length_e = max(major_axis, minor_axis)\n",
    "            angle_e = ellipse[2]\n",
    "\n",
    "             # Compute Convex Hull & its Area\n",
    "            hull = cv2.convexHull(ct)\n",
    "            hull_area = cv2.contourArea(hull)       \n",
    "            # Compute Solidity\n",
    "            solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "            data_list.append([i, j, cntx, cnty, area, solidity, length_rect, aspect_ratio, angle_rect, length_e, angle_e])\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"frame\", \"spot_ID\", \"x\", \"y\", \"area\", \"solidity\", \n",
    "           \"rect_length\", \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\"]\n",
    "df = pd.DataFrame(data_list, columns=columns)\n",
    "\n",
    "# Save DataFrame as a CSV file\n",
    "df.to_csv(dataFpath, index=False)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77fecdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Match data between contour DataFrame and tracks from TrackMate\n",
    "----input----\n",
    "trackmate/-BG_spots.csv, -spots_cv2.csv\n",
    "\n",
    "----output----\n",
    "-spots_cv2-tmmap.csv\n",
    "\"\"\"\n",
    "from scipy.spatial import KDTree\n",
    "exppath = 'ForPub/150-LBLMagar/250717-150-2-ana/'\n",
    "# exppath = 'ForPub/075-LBLMagar/250117-075-2-ana/'\n",
    "rplcpathList = [f.path for f in os.scandir(exppath) if f.is_dir() and '00955' in f.name]\n",
    "for rplcpath in sorted(rplcpathList):\n",
    "    head, rplcIdx = os.path.split(rplcpath) \n",
    "\n",
    "    tmpath = rplcpath + '/trackmate'\n",
    "    spotsName =  rplcIdx + \"-BG_spots.csv\" \n",
    "    spotspath = os.path.join(tmpath, spotsName)\n",
    "    dataFName = rplcIdx + '-spots_cv2.csv'\n",
    "    dataFpath = os.path.join(tmpath, dataFName)\n",
    "    outName = rplcIdx + '-spots_cv2_tmmap.csv'\n",
    "    outpath = os.path.join(tmpath, outName)\n",
    "\n",
    "    # Load TrackMate and opencv results\n",
    "    trackmate_df = pd.read_csv(spotspath, skiprows=[1, 2, 3], header=0, usecols =[\"ID\", \"POSITION_X\",\"POSITION_Y\", \"FRAME\"])  \n",
    "    opencv_df = pd.read_csv(dataFpath)\n",
    "\n",
    "    # Create a dictionary of KDTree per frame using TrackMate spots\n",
    "    frame_trees = {}\n",
    "    for frame, group in trackmate_df.groupby(\"FRAME\"):\n",
    "        # Build KDTree for TrackMate spots in this frame\n",
    "        frame_trees[frame] = (KDTree(group[[\"POSITION_X\", \"POSITION_Y\"]]), group)\n",
    "\n",
    "\n",
    "    # Initialize the new column in OpenCV DataFrame\n",
    "    opencv_df[\"tm_spot_ID\"] = None  \n",
    "    search_radius = 10\n",
    "\n",
    "    # Map opencv contour to the nearest trackmate spot\n",
    "    for i, row in opencv_df.iterrows():\n",
    "        frame, x, y = row['frame'], row['x'], row['y']\n",
    "        if frame in frame_trees:\n",
    "            tree, frame_data = frame_trees[frame]\n",
    "            dist, idx = tree.query([x, y], distance_upper_bound=search_radius)\n",
    "\n",
    "            # Only assign a match if the nearest TrackMate spot is within the threshold\n",
    "            if dist < search_radius:\n",
    "                trackmate_spot = frame_data.iloc[idx]\n",
    "                opencv_df.at[i, \"tm_spot_ID\"] = int(trackmate_spot[\"ID\"])\n",
    "\n",
    "    # Save mapping results\n",
    "    opencv_df.to_csv(outpath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b489cd-ee8b-402d-8c3a-29ae8b9fa3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frame  spot_ID           x           y    area  solidity  rect_length  \\\n",
      "0      0        0  410.980234  395.268793  1113.0  0.918317    71.059067   \n",
      "1      1        0  405.441001  389.843882  1145.5  0.916400    73.095490   \n",
      "2      2        0  404.394541  387.734769  1184.5  0.922508    72.622131   \n",
      "3      3        0  403.521800  386.046525  1196.5  0.909886    74.651062   \n",
      "4      4        0  402.726996  384.930262  1204.5  0.907003    76.394119   \n",
      "\n",
      "   rect_aspect_ratio  rect_angle  ellipse_long_axis  ellipse_theta tm_spot_ID  \n",
      "0           3.729508   38.659805          81.430725     128.840820       8896  \n",
      "1           3.795306   38.884495          83.303757     129.393982       8903  \n",
      "2           3.608054   38.418053          84.139160     129.189056       8892  \n",
      "3           3.793651   38.659809          86.977219     128.899750       8895  \n",
      "4           3.854569   37.746803          90.296951     129.144623       8893  \n"
     ]
    }
   ],
   "source": [
    "print(opencv_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df53bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 00955 spot_ID 17067.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 17240.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 16121 not found in spots_df.\n",
      "Warning: 00955 spot_ID 16091 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20035.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20075.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20028.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20059.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 21558.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 19609 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20166.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20023.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25600.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25349.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25405.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 20908 not found in spots_df.\n",
      "Warning: 00955 spot_ID 22775.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 21054 not found in spots_df.\n",
      "Warning: 00955 spot_ID 22850.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 21610 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25771.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23952.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23423.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 22360 not found in spots_df.\n",
      "Warning: 00955 spot_ID 22189 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23900.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25922.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 26069.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 26497.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23775 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25032 not found in spots_df.\n",
      "Warning: 00955 spot_ID 26390.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23315 not found in spots_df.\n",
      "Warning: 00955 spot_ID 23193 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25803.0 not found in spots_df.\n",
      "Warning: 00955 spot_ID 27332 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25650 not found in spots_df.\n",
      "Warning: 00955 spot_ID 25979 not found in spots_df.\n",
      "Warning: 00955 spot_ID 24246 not found in spots_df.\n",
      "Warning: 00955 spot_ID 24162 not found in spots_df.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reconstruct cell tracks from \"edge.csv\" given by TrackMate and opencv analysis\n",
    "only cells roots back to begining\n",
    "\n",
    "----input----\n",
    "\"_edges.csv\" : [SPOT_SOURCE_ID,SPOT_TARGET_ID,EDGE_TIME]\n",
    "'-spots_cv2_tmmap.csv': [\"frame\", \"spot_ID\", \"x\", \"y\", \"area\", \"solidity\", \n",
    "\"rect_length\", \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\", \"tm_spot_ID\"]\n",
    "\n",
    "---Output---\n",
    "Tracks DataFrame\n",
    " [\"cell_ID\", \"cell_parent_ID\", \"cell_daughter1_ID\", \"cell_daughter2_ID\", \"lineage\", \"generation\", \"tm_spot_ID\", \"frame\", \n",
    " \"x\", \"y\", \"area\", \"solidity\", \"rect_length\", \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\"]\n",
    "\n",
    " _Note:_ not all contours has tm_spot_ID\n",
    "\"\"\"\n",
    "# exppath = 'ForPub/075-LBLMagar/241219-075-ana/'\n",
    "# rplcpathList = [f.path for f in os.scandir(exppath) if f.is_dir() and '00' in f.name]\n",
    "# rplcpathList = ['ForPub/150-LBLMagar/240228-150-1-ana/00321-2']\n",
    "\n",
    "for rplcpath in sorted(rplcpathList):\n",
    "    head, rplcIdx = os.path.split(rplcpath) \n",
    "    \n",
    "    # rplcpath ='ForPub/600-LBLMagar/00798-2'\n",
    "    # head, rplcIdx = os.path.split(rplcpath)\n",
    "    tmpath = rplcpath + '/trackmate'\n",
    "    edgesName =  rplcIdx + \"-BG_edges.csv\" \n",
    "    edgespath = os.path.join(tmpath, edgesName)\n",
    "    spotsName =  rplcIdx + '-spots_cv2_tmmap.csv'\n",
    "    spotspath = os.path.join(tmpath, spotsName)\n",
    "    tracksName = rplcIdx + \"-tracks-cv2-raw.csv\" \n",
    "    trackspath = os.path.join(rplcpath, tracksName)\n",
    "\n",
    "\n",
    "    edges_df = pd.read_csv(edgespath, skiprows=[1, 2, 3], header=0, \n",
    "                           usecols =[\"SPOT_SOURCE_ID\", \"SPOT_TARGET_ID\", \"EDGE_TIME\"] )\n",
    "    spots_df = pd.read_csv(spotspath)\n",
    "\n",
    "    # Ensure data types are correct\n",
    "    edges_df[\"SPOT_SOURCE_ID\"] = edges_df[\"SPOT_SOURCE_ID\"].astype(int)\n",
    "    edges_df[\"SPOT_TARGET_ID\"] = edges_df[\"SPOT_TARGET_ID\"].astype(int)\n",
    "    edges_df[\"EDGE_TIME\"] = edges_df[\"EDGE_TIME\"].astype(float)  # Ensure numeric\n",
    "    # spots_df[\"tm_spot_ID\"] = spots_df[\"tm_spot_ID\"].astype(int) \n",
    "    # spots_df[\"frame\"] = spots_df[\"frame\"].astype(int)\n",
    "\n",
    "    # Sort dataframes to process tracks sequentially\n",
    "    edges_df = edges_df.sort_values(by=\"EDGE_TIME\").reset_index(drop=True)\n",
    "    spots_df = spots_df.sort_values(by=\"frame\").reset_index(drop=True)\n",
    "\n",
    "    # Initialize tracking structures\n",
    "    cell_tracks = {}  # {cell_ID: [list of spot_IDs]}\n",
    "    cell_map = {}  # {spot_ID: cell_ID}\n",
    "    parent_map = {}  # {cell_ID -> parent_ID}  \n",
    "    daughter_map = {}  # {cell_ID: [daughter1_ID, daughter2_ID]}\n",
    "\n",
    "    # Identify root cells (cells in the first frame, can handle multiple cells)\n",
    "    first_frame = edges_df[\"EDGE_TIME\"].min() - 0.5\n",
    "    root_edges = edges_df[edges_df[\"EDGE_TIME\"] == first_frame+0.5]   \n",
    "\n",
    "    cell_counter = 0\n",
    "    for _, row in root_edges.iterrows():\n",
    "        source_spot = row[\"SPOT_SOURCE_ID\"]\n",
    "        target_spot = row[\"SPOT_TARGET_ID\"]\n",
    "\n",
    "        # Assign cell_IDs\n",
    "        cell_map[source_spot] = cell_counter\n",
    "        cell_map[target_spot] = cell_counter\n",
    "        parent_map[cell_counter] = None  # Root cells have no parent\n",
    "        cell_tracks[cell_counter] = [source_spot, target_spot]\n",
    "\n",
    "        cell_counter += 1\n",
    "\n",
    "    # Process edges to extend tracks and detect division\n",
    "    for _, row in edges_df.iterrows():\n",
    "        # Check for division\n",
    "        source_spot = row[\"SPOT_SOURCE_ID\"]\n",
    "        target_spot = row[\"SPOT_TARGET_ID\"]\n",
    "        frame_id = int(row[\"EDGE_TIME\"] - 0.5)  # Convert EDGE_TIME to integer frame    \n",
    "\n",
    "        children = edges_df[edges_df[\"SPOT_SOURCE_ID\"] == source_spot][\"SPOT_TARGET_ID\"].tolist() # list of targetID  \n",
    "        if len(children) == 1:  \n",
    "            if source_spot in cell_map and not(target_spot in cell_map):\n",
    "                cell_id = cell_map[source_spot]  #cell_id, also used for division\n",
    "                # Extend track by linking TARGET to SOURCE in the next frame\n",
    "                cell_map[target_spot] = cell_id\n",
    "                cell_tracks[cell_id].append(target_spot)\n",
    "\n",
    "        elif len(children) == 2: # handle two children at the same time, should avoid repeat\n",
    "            if source_spot in cell_map and not(target_spot in cell_map):\n",
    "                parent_id = cell_map[source_spot]\n",
    "    #             print(f\"Division detected at frame {frame_id} for cell {parent_id} {source_spot} → daughters: {children}\")\n",
    "\n",
    "                daughter1_id = cell_counter\n",
    "                daughter2_id = cell_counter + 1\n",
    "\n",
    "                cell_map[children[0]] = daughter1_id\n",
    "                cell_map[children[1]] = daughter2_id\n",
    "                parent_map[daughter1_id] = parent_id\n",
    "                parent_map[daughter2_id] = parent_id\n",
    "                daughter_map[parent_id] = [daughter1_id, daughter2_id]\n",
    "\n",
    "                cell_tracks[daughter1_id] = [children[0]]\n",
    "                cell_tracks[daughter2_id] = [children[1]]\n",
    "                cell_counter += 2\n",
    "        else:\n",
    "            print(source_spot, f\"abnormal\")\n",
    "\n",
    "\n",
    "    #build lineage from parent_map \n",
    "    lineage_map = {}  # {cell_ID -> whole lineage from root cell} \n",
    "    for cell_id in cell_tracks.keys():\n",
    "        lineage_map[int(cell_id)] = [cell_id]\n",
    "    for cell_id in lineage_map.keys():\n",
    "        parent_id = parent_map[cell_id]\n",
    "        if parent_id is not None:  # If not a root cell\n",
    "            lineage_map[cell_id] = lineage_map[int(parent_id)] + [cell_id]\n",
    "        else:\n",
    "            lineage_map[cell_id] = [cell_id] \n",
    "    # Print tracking results for debugging\n",
    "    # print(\"\\nFinal cell tracks:\")\n",
    "    # for cell_id, track in cell_tracks.items():\n",
    "    #     print(f\"Cell {cell_id}: {track}\")\n",
    "\n",
    "\n",
    "    # Convert track data into DataFrame format\n",
    "     # [\"cell_ID\", \"cell_parent_ID\", \"cell_daughter1_ID\", \"cell_daughter2_ID\", \"lineage\", \"generation\", \"tm_spot_ID\", \"frame\", \n",
    "     # \"x\", \"y\", \"area\", \"solidity\", \"rect_length\", \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\"]\n",
    "    lineage_data = []\n",
    "    for cell_id, spot_list in cell_tracks.items():\n",
    "       for spot in spot_list:\n",
    "            # Select relevant row(s) for the current spot_ID\n",
    "            spot_data = spots_df.loc[spots_df[\"tm_spot_ID\"] == spot, \n",
    "                [\"frame\", \"x\", \"y\", \"area\", \"solidity\", \"rect_length\", \n",
    "                 \"rect_aspect_ratio\", \"rect_angle\", \"ellipse_long_axis\", \"ellipse_theta\"]]\n",
    "\n",
    "            # Only proceed if there is a matching row\n",
    "            if not spot_data.empty:  \n",
    "                lineage_data.append({\n",
    "                    \"cell_ID\": cell_id,\n",
    "                    \"cell_parent_ID\": parent_map.get(cell_id),\n",
    "                    \"cell_daughter1_ID\": daughter_map.get(cell_id, [None, None])[0],\n",
    "                    \"cell_daughter2_ID\": daughter_map.get(cell_id, [None, None])[1],\n",
    "                    \"lineage\": lineage_map[cell_id],\n",
    "                    \"generation\": len(lineage_map[cell_id])-1,\n",
    "                    \"spot_ID\": spot,\n",
    "                    **spot_data.iloc[0].to_dict().copy()  # Convert row to dictionary safely\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: {rplcIdx} spot_ID {spot} not found in spots_df.\") \n",
    "\n",
    "    # Convert to DataFrame\n",
    "    lineage_df = pd.DataFrame(lineage_data)\n",
    "\n",
    "    # Display results\n",
    "    # print(lineage_df.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    lineage_df.to_csv(trackspath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5f034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4dc3c50-863b-4864-8b99-570cf67654bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "250306 modified from ChatGPT script\n",
    "\n",
    "Reconstruct cell tracks from \"edge.csv\" given by TrackMate\n",
    "The output datafile \n",
    "----input----\n",
    "\"_edges.csv\" : [SPOT_SOURCE_ID,SPOT_TARGET_ID,EDGE_TIME]\n",
    "\"_spots.csv\": [ID, POSITION_X,POSITION_Y, FRAME, ELLIPSE_MAJOR,ELLIPSE_THETA, SOLIDITY]\n",
    "\n",
    "---Output---\n",
    "Tracks DataFrame\n",
    " [\"cell_ID\", \"cell_parent_ID\", \"cell_daughter1_ID\", \"cell_daughter2_ID\", \"lineage\", \"generation\"\n",
    " \"spot_ID\", \"frame\", \"x\", \"y\", \"ellipse_long_axis\", \"ellipse_theta\", \"solidity\" ]\n",
    " \n",
    "Note:\n",
    "in TrackMate spot analysis, \"ELLIPSE_MAJOR\" may not be the long axis\n",
    "\"\"\"\n",
    "rplcpath ='ForPub/075-LBLMagar/00823'\n",
    "head, rplcIdx = os.path.split(rplcpath)\n",
    "tmpath = rplcpath + '/trackmate'\n",
    "\n",
    "edgesName =  rplcIdx + \"-BG_edges.csv\" \n",
    "edgespath = os.path.join(tmpath, edgesName)\n",
    "spotsName =  rplcIdx + \"-BG_spots.csv\" \n",
    "spotspath = os.path.join(tmpath, spotsName)\n",
    "tracksName = rplcIdx + \"-tracks-tm.csv\" \n",
    "trackspath = os.path.join(rplcpath, tracksName)\n",
    "\n",
    "\n",
    "edges_df = pd.read_csv(edgespath, skiprows=[1, 2, 3], header=0, \n",
    "                       usecols =[\"SPOT_SOURCE_ID\", \"SPOT_TARGET_ID\", \"EDGE_TIME\"] )\n",
    "spots_df = pd.read_csv(spotspath, skiprows=[1, 2, 3], header=0, \n",
    "                       usecols =[\"ID\", \"POSITION_X\",\"POSITION_Y\", \"FRAME\", \"ELLIPSE_MAJOR\",\"ELLIPSE_MINOR\",\"ELLIPSE_THETA\", \"SOLIDITY\"] )\n",
    "\n",
    "# Ensure data types are correct\n",
    "edges_df[\"SPOT_SOURCE_ID\"] = edges_df[\"SPOT_SOURCE_ID\"].astype(int)\n",
    "edges_df[\"SPOT_TARGET_ID\"] = edges_df[\"SPOT_TARGET_ID\"].astype(int)\n",
    "edges_df[\"EDGE_TIME\"] = edges_df[\"EDGE_TIME\"].astype(float)  # Ensure numeric\n",
    "spots_df[\"ID\"] = spots_df[\"ID\"].astype(int)\n",
    "spots_df[\"FRAME\"] = spots_df[\"FRAME\"].astype(int)\n",
    "\n",
    "# Sort dataframes to process tracks sequentially\n",
    "edges_df = edges_df.sort_values(by=\"EDGE_TIME\").reset_index(drop=True)\n",
    "spots_df = spots_df.sort_values(by=\"FRAME\").reset_index(drop=True)\n",
    "\n",
    "# Initialize tracking structures\n",
    "cell_tracks = {}  # {cell_ID: [list of spot_IDs]}\n",
    "cell_map = {}  # {spot_ID: cell_ID}\n",
    "parent_map = {}  # {cell_ID -> parent_ID}  \n",
    "daughter_map = {}  # {cell_ID: [daughter1_ID, daughter2_ID]}\n",
    "        \n",
    "# Identify root cells (cells in the first frame, can handle multiple cells)\n",
    "first_frame = edges_df[\"EDGE_TIME\"].min() - 0.5\n",
    "root_edges = edges_df[edges_df[\"EDGE_TIME\"] == first_frame+0.5]   \n",
    "\n",
    "cell_counter = 0\n",
    "for _, row in root_edges.iterrows():\n",
    "    source_spot = row[\"SPOT_SOURCE_ID\"]\n",
    "    target_spot = row[\"SPOT_TARGET_ID\"]\n",
    "\n",
    "    # Assign cell_IDs\n",
    "    cell_map[source_spot] = cell_counter\n",
    "    cell_map[target_spot] = cell_counter\n",
    "    parent_map[cell_counter] = None  # Root cells have no parent\n",
    "    cell_tracks[cell_counter] = [source_spot, target_spot]\n",
    "\n",
    "    cell_counter += 1\n",
    "\n",
    "# Process edges to extend tracks and detect division\n",
    "for _, row in edges_df.iterrows():\n",
    "    # Check for division\n",
    "    source_spot = row[\"SPOT_SOURCE_ID\"]\n",
    "    target_spot = row[\"SPOT_TARGET_ID\"]\n",
    "    frame_id = int(row[\"EDGE_TIME\"] - 0.5)  # Convert EDGE_TIME to integer frame    \n",
    "    \n",
    "    children = edges_df[edges_df[\"SPOT_SOURCE_ID\"] == source_spot][\"SPOT_TARGET_ID\"].tolist() # list of targetID  \n",
    "    if len(children) == 1:  \n",
    "        if source_spot in cell_map and not(target_spot in cell_map):\n",
    "            cell_id = cell_map[source_spot]  #cell_id, also used for division\n",
    "            # Extend track by linking TARGET to SOURCE in the next frame\n",
    "            cell_map[target_spot] = cell_id\n",
    "            cell_tracks[cell_id].append(target_spot)\n",
    "\n",
    "    elif len(children) == 2: # handle two children at the same time, should avoid repeat\n",
    "        if source_spot in cell_map and not(target_spot in cell_map):\n",
    "            parent_id = cell_map[source_spot]\n",
    "#             print(f\"Division detected at frame {frame_id} for cell {parent_id} {source_spot} → daughters: {children}\")\n",
    "\n",
    "            daughter1_id = cell_counter\n",
    "            daughter2_id = cell_counter + 1\n",
    "\n",
    "            cell_map[children[0]] = daughter1_id\n",
    "            cell_map[children[1]] = daughter2_id\n",
    "            parent_map[daughter1_id] = parent_id\n",
    "            parent_map[daughter2_id] = parent_id\n",
    "            daughter_map[parent_id] = [daughter1_id, daughter2_id]\n",
    "\n",
    "            cell_tracks[daughter1_id] = [children[0]]\n",
    "            cell_tracks[daughter2_id] = [children[1]]\n",
    "            cell_counter += 2\n",
    "    else:\n",
    "        print(source_spot, f\"abnormal\")\n",
    "\n",
    "\n",
    "#build lineage from parent_map \n",
    "lineage_map = {}  # {cell_ID -> whole lineage from root cell} \n",
    "for cell_id in cell_tracks.keys():\n",
    "    lineage_map[int(cell_id)] = [cell_id]\n",
    "for cell_id in lineage_map.keys():\n",
    "    parent_id = parent_map[cell_id]\n",
    "    if parent_id is not None:  # If not a root cell\n",
    "        lineage_map[cell_id] = lineage_map[int(parent_id)] + [cell_id]\n",
    "    else:\n",
    "        lineage_map[cell_id] = [cell_id] \n",
    "# Print tracking results for debugging\n",
    "# print(\"\\nFinal cell tracks:\")\n",
    "# for cell_id, track in cell_tracks.items():\n",
    "#     print(f\"Cell {cell_id}: {track}\")\n",
    "            \n",
    "            \n",
    "# Convert track data into DataFrame format\n",
    "# [\"cell_ID\", \"cell_parent_ID\", \"cell_daughter1_ID\", \"cell_daughter2_ID\", \"lineage\",\n",
    "#  \"spot_ID\", \"frame\", \"x\", \"y\", \"ellipse_long_axis\", \"ellipse_theta\", \"solidity\" ]\n",
    "lineage_data = []\n",
    "for cell_id, spot_list in cell_tracks.items():\n",
    "    for spot in spot_list:\n",
    "#         frame = edges_df.loc[edges_df[\"SPOT_SOURCE_ID\"] == spot, \"EDGE_TIME\"].min()\n",
    "#         frame_id = frame - 0.5 if not pd.isna(frame) else None #TODO: add last\n",
    "        lineage_data.append({\n",
    "            \"cell_ID\": cell_id,\n",
    "            \"cell_parent_ID\": parent_map[cell_id],\n",
    "            \"cell_daughter1_ID\": daughter_map[cell_id][0] if cell_id in daughter_map else None,\n",
    "            \"cell_daughter2_ID\": daughter_map[cell_id][1] if cell_id in daughter_map else None,\n",
    "            \"lineage\": lineage_map[cell_id],\n",
    "            \"generation\": len(lineage_map[cell_id])-1,\n",
    "            \"spot_ID\": spot,\n",
    "            \"frame\": spots_df.loc[spots_df[\"ID\"] == spot, \"FRAME\"].iloc[0],\n",
    "            \"x\": spots_df.loc[spots_df[\"ID\"] == spot, \"POSITION_X\"].iloc[0],\n",
    "            \"y\": spots_df.loc[spots_df[\"ID\"] == spot, \"POSITION_Y\"].iloc[0],\n",
    "            \"ellipse_long_axis\": spots_df.loc[spots_df[\"ID\"] == spot, \"ELLIPSE_MAJOR\"].iloc[0],  # need revise\n",
    "            \"ellipse_theta\": spots_df.loc[spots_df[\"ID\"] == spot, \"ELLIPSE_THETA\"].iloc[0],\n",
    "            \"solidity\": spots_df.loc[spots_df[\"ID\"] == spot, \"SOLIDITY\"].iloc[0]\n",
    "        })\n",
    "# usecols =[\"ID\", \"POSITION_X\",\"POSITION_Y\", \"FRAME\", \"ELLIPSE_MAJOR\",\"ELLIPSE_THETA\", \"SOLIDITY\"] \n",
    "    \n",
    "# Convert to DataFrame\n",
    "lineage_df = pd.DataFrame(lineage_data)\n",
    "\n",
    "# Display results\n",
    "# print(lineage_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "lineage_df.to_csv(trackspath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "910177c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [0, 1], 2: [0, 2], 3: [0, 1, 3], 4: [0, 1, 4]}\n"
     ]
    }
   ],
   "source": [
    "lineage_map = {0: [0], 1: [1], 2: [2], 3: [3], 4: [4]}\n",
    "parent_map = {0: np.nan, 1: 0, 2: 0, 3: 1, 4: 1}\n",
    "\n",
    "for cell_id in lineage_map.keys():\n",
    "    parent_id = parent_map[cell_id]\n",
    "    if not np.isnan(parent_id):  # If not a root cell\n",
    "        lineage_map[cell_id] = lineage_map[int(parent_id)] + [cell_id]\n",
    "    else:\n",
    "        lineage_map[cell_id] = [cell_id] \n",
    "print(lineage_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68845fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForPub/300-LBLMagar/00766\\00766-dataDic.pkl\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1st version, cv2 contour analysis, result given in Dictionary of numpy array\n",
    "\n",
    "-----Input----\n",
    "Ilastik segmented, binary image series. Cells must be separated.\n",
    "-----Output-----\n",
    "Dictionary: [frameIdx]: nparray #  cellIdx, cntx, cnty, 3rect0, 4rect1, 5area, 6aspect ratio(AR), 7length, 8angle\n",
    "\"\"\"\n",
    "\n",
    "fpath = os.path.join(rplcpath, segName)\n",
    "cnpath = os.path.join(rplcpath, dataDicName)\n",
    "print(cnpath)\n",
    "\n",
    "img_segs = []\n",
    "ret,img_segs = cv2.imreadmulti(mats=img_segs, \n",
    "                               filename=fpath, flags=cv2.IMREAD_GRAYSCALE)\n",
    "print(len(img_segs))\n",
    "dataDic = {} # empty dictionary [frameIdx]: fData\n",
    "for i in range(len(img_segs)):\n",
    "    img_seg = img_segs[i]\n",
    "#     # convert 1 2 label to 0 and 255\n",
    "#     one = np.array(1)\n",
    "#     img_seg = np.subtract(img_seg, one)\n",
    "#     img_seg = img_seg *255 \n",
    "\n",
    "#     # agar movies are black on white\n",
    "#     inv = np.array(255)\n",
    "#     img_seg = np.subtract(inv, img_seg)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(img_seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    fData = np.array([]).reshape(0,9)  # cellIdx, cntx, cnty, 3rect0, 4rect1, 5area, 6aspect ratio(AR), 7length, 8angle\n",
    "    for j,ct in enumerate(contours):\n",
    "        area = cv2.contourArea(ct)\n",
    "        cellData = np.zeros(9)       \n",
    "        if area > 100 and area < 5000:   # size range for cells \n",
    "            cellData[0] = j\n",
    "            cellData[5] = area\n",
    "            M = cv2.moments(ct)\n",
    "            cellData[1] = M['m10']/M['m00']  # cntx    \n",
    "            cellData[2] = M['m01']/M['m00']  # cnty\n",
    "            \n",
    "            peri = cv2.arcLength(ct,True)\n",
    "#             approx = cv2.approxPolyDP(cnt,0.00009*peri,True)\n",
    "            # bounding box to get length, width, AR, angle\n",
    "            rect = cv2.minAreaRect(ct)\n",
    "#             box = cv2.boxPoints(rect)\n",
    "#             box = np.int0(box)\n",
    "            # Retrieve the key parameters of the rotated bounding box\n",
    "#             center = (int(rect[0][0]),int(rect[0][1])) \n",
    "            cellData[3] = rect[1][0]           \n",
    "            cellData[4] = rect[1][1]   \n",
    "            if cellData[3] >= cellData[4]:\n",
    "                cellData[6] = cellData[3] / cellData[4]  # length / width\n",
    "                cellData[7] = cellData[3]\n",
    "            else:\n",
    "                cellData[6] = cellData[4] / cellData[3] \n",
    "                cellData[7] = cellData[4]\n",
    "            cellData[8] = rect[2]   # angle\n",
    "            fData = np.vstack((fData, cellData))\n",
    "            \n",
    "    dataDic[str(i)] = fData\n",
    "#     print(len(contours))\n",
    "#     print(i, j, fData)\n",
    "\n",
    "with open(cnpath, 'wb') as f:\n",
    "    pickle.dump(dataDic, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "452db423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "old Trajectory (cell lineage) analysis for agar, with ellipse \n",
    "----\n",
    "Input:\n",
    "    Dictionary: [frameIdx]: nparray # cellIdx, cntx, cnty, length, width, aspect ratio(AR), area, angle\n",
    "Output:\n",
    "    1. array for split events: # fIdx, Mother cell Idx, M length, Daughter cell Idx1, D1 length, D2 Idx, D2 length\n",
    "    2. Trajectories: # fIdx, cellIdx, cntx, cnty, length, area, angle\n",
    "----\n",
    "Find all the cells on the next frame overlap with the cell on the previous frame, cnt distance < 0.4(?) cell length\n",
    "(images aligned in 1st step)\n",
    "0 cell: close traj; 1 cell: traj+1; 2 cells: start 2 new trajs, record a division.\n",
    "Not include: (1) find >2 cells within threshold\n",
    "\n",
    "---20240205 update---\n",
    "ellipse 2a, 2b, angle following rect, cnt follows contour cnt,\n",
    "Find daughter cells based on if new cnt is inside elipse\n",
    "\n",
    "---20240421 update---\n",
    "include traj index into div file\n",
    "---20250304 revised by ChatGPT, bug fixed for counting trajs multiple times---\n",
    "\n",
    "\"\"\"\n",
    "# exppath = 'glass-LBLMagar-003/240620-003LBLMagar-ana/'  #240529, 240530, 240612, 240619, 240620\n",
    "# exppath = 'glass-LBLMagar-0075/240618-0075LBLMagar-ana/'  # 240530, 240613, 240618\n",
    "# exppath = 'glass-LBLMagar-010/240613-010LBLMagar-ana/' # 240613, 240618\n",
    "# exppath = 'glass-LBLMagar-015/240228-agar-GF-ana/'  #240228 240307\n",
    "# exppath = 'LB-glass-coverslip/240229-glass-GF-ana/'\n",
    "# exppath = 'glass-LBLMagar/Elongations/240307/'  #   240307 231006-60x\n",
    "\n",
    "# rplcpathList = [f.path for f in os.scandir(exppath) if f.is_dir()]\n",
    "\n",
    "# rplcpath = rplcpathList[0]\n",
    "# rplcpath = 'glass-LBLMagar-015/240307-agar-GF-ana/00392-3'\n",
    "# head, rplcIdx = os.path.split(rplcpath) \n",
    "\n",
    "# nbgName = rplcIdx + '-BG.tif'\n",
    "# # prepName = dataIdx + '-BG-prep.tif'\n",
    "# segName = rplcIdx + '-BG_Simple Segmentation.tiff' \n",
    "# dataDicName = rplcIdx + '-dataDic.pkl'\n",
    "# divName = rplcIdx + '-Div.txt'\n",
    "# trajDicName = rplcIdx + '-trajDic.pkl'\n",
    "\n",
    "# fpath = os.path.join(rplcpath, segName)\n",
    "cnpath = os.path.join(rplcpath, dataDicName)\n",
    "trajpath = os.path.join(rplcpath, trajDicName)\n",
    "divpath =  os.path.join(rplcpath, divName)\n",
    "\n",
    "# dist = np.linalg.norm(p1-p2)  \n",
    "# 2a = rect[1][0], 2b = rect[1][1], angle = rect[2], alpha = angle (in rad)\n",
    "def inEllipse(x, y, a, b, m, n, alpha):\n",
    "    term1 = ((x-m)*np.cos(alpha) + (y-n)*np.sin(alpha))**2 /(a**2)\n",
    "    term2 = (-(x-m)*np.sin(alpha) + (y-n)*np.cos(alpha))**2 /(b**2)\n",
    "    if term1 + term2 < 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "with open(cnpath, 'rb') as f:\n",
    "    dataDic = pickle.load(f)\n",
    "trajDic = {}\n",
    "division = np.array([]).reshape(0,7)\n",
    "Header = '# 0fIdx, 1Mother cell Idx, 2M length, 3Daughter cell Idx1, 4D1 length, 5Daughter cell Idx2, 6D2 length'\n",
    "\n",
    "# start trajs with all cells in frame#0\n",
    "fData0 = dataDic[str(0)]  # cellIdx, cntx, cnty, 3rect0, 4rect1, 5area, 6aspect ratio(AR), 7length, 8angle\n",
    "for i in range(fData0.shape[0]):\n",
    "    traj = np.zeros([1,10]) # 0fIdx, 1cellIdx, 2cntx, 3cnty, 4rect0, 5rect1, 6area, 7aspect ratio(AR), 8length, 9angle\n",
    "#     traj[0] = 0\n",
    "    traj[0, 1:] = fData0[i, 0:]\n",
    "    trajDic[str(i)] = traj\n",
    "    \n",
    "# Process frames sequentially\n",
    "trajExKeys = list(trajDic.keys())  # Keys for ongoing trajectories\n",
    "\n",
    "for k in range(1, len(dataDic.keys())):  # Read frames in order\n",
    "    fData = dataDic[str(k)]  # Read current frame\n",
    "    ttrajExKeys = trajExKeys.copy()\n",
    "    used_cells = set()  # Track used cell indices in the new frame\n",
    "\n",
    "    for j in trajExKeys:  # Loop through existing trajectories\n",
    "        traj = trajDic[j]\n",
    "        endData = traj[-1, :]  # Last known cell state\n",
    "\n",
    "        # Extract ellipse parameters from the last frame\n",
    "        cntX0, cntY0 = endData[2], endData[3]\n",
    "        aa, bb = endData[4] *0.6, endData[5] *0.6  # Semi-major and semi-minor axes 250304 relax to 0.7\n",
    "        aalpha = np.radians(endData[-1])  # Convert angle to radians\n",
    "        dCounter = 0  # Track daughter cells\n",
    "        daughter_cells = []\n",
    "\n",
    "        for i in range(fData.shape[0]):  # Loop through new frame cells\n",
    "            if i in used_cells:\n",
    "                continue  # Skip cells already assigned to a trajectory\n",
    "\n",
    "            cntX1, cntY1 = fData[i, 1], fData[i, 2]  # New cell center\n",
    "\n",
    "            if inEllipse(cntX1, cntY1, aa, bb, cntX0, cntY0, aalpha):\n",
    "                dCounter += 1\n",
    "                daughter_cells.append(i)\n",
    "\n",
    "                if dCounter == 1:  # First match -> extend trajectory\n",
    "                    traj1 = np.zeros([1, 10])\n",
    "                    traj1[0, 0] = k  # Frame index\n",
    "                    traj1[0, 1:] = fData[i, :]\n",
    "                    traj = np.vstack((traj, traj1))  # Extend trajectory\n",
    "                    used_cells.add(i)  # Mark cell as used\n",
    "\n",
    "                elif dCounter == 2:  # Second match -> division event\n",
    "                    # Record division event\n",
    "                    nDiv = np.zeros(7)\n",
    "                    nDiv[0] = k - 1  # Frame of mother cell\n",
    "                    nDiv[1] = endData[1]  # Mother cell ID\n",
    "                    nDiv[2] = endData[-2]  # Mother cell length\n",
    "                    nDiv[3] = fData[daughter_cells[0], 0]  # Daughter 1 ID\n",
    "                    nDiv[4] = fData[daughter_cells[0], -2]  # Daughter 1 length\n",
    "                    nDiv[5] = fData[daughter_cells[1], 0]  # Daughter 2 ID\n",
    "                    nDiv[6] = fData[daughter_cells[1], -2]  # Daughter 2 length\n",
    "                    division = np.vstack((division, nDiv))\n",
    "\n",
    "                    # Close old trajectory\n",
    "                    ttrajExKeys.remove(j)\n",
    "\n",
    "                    # Start new trajectories for daughter cells\n",
    "                    for dIdx in daughter_cells:\n",
    "                        new_key = str(len(trajDic))  # Generate new trajectory key\n",
    "                        new_traj = np.zeros([1, 10])\n",
    "                        new_traj[0, 0] = k  # Frame index\n",
    "                        new_traj[0, 1:] = fData[dIdx, :]\n",
    "                        trajDic[new_key] = new_traj\n",
    "                        ttrajExKeys.append(new_key)\n",
    "                        used_cells.add(dIdx)  # Mark cell as used\n",
    "\n",
    "                    break  # Exit loop once division is recorded\n",
    "\n",
    "        if dCounter == 0:  # If no cell found, remove trajectory from active list\n",
    "            ttrajExKeys.remove(j)\n",
    "        \n",
    "        trajDic[j] = traj  # Update trajectory dictionary\n",
    "\n",
    "    trajExKeys = ttrajExKeys.copy()  # Update active trajectory keys\n",
    "    \n",
    "with open(trajpath, 'wb') as f:\n",
    "    pickle.dump(trajDic, f)\n",
    "f.close()\n",
    "np.savetxt(divpath, division, fmt='%d', header=Header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8534cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"track plot check\n",
    "write traj ID on to segmented tif\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "with open(trajpath, 'rb') as f:\n",
    "    trajDic = pickle.load(f)  # 0fIdx, 1cellIdx, 2cntx, 3cnty, 4rect0, 5rect1, 6area, 7aspect ratio(AR), 8length, 9angle\n",
    "    \n",
    "fpath = os.path.join(rplcpath, segName)\n",
    "outName = rplcIdx + '-trajCheck.tif'\n",
    "outpath = os.path.join(rplcpath, outName)\n",
    "frames = []\n",
    "frames_processed = []\n",
    "\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale              = 0.7\n",
    "fontColor              = (255,0,0)\n",
    "thickness              = 2\n",
    "lineType               = 2\n",
    "\n",
    "ret,frames = cv2.imreadmulti(mats=frames, filename=fpath, \n",
    "#                               start=0,\n",
    "#                               count=20,                              \n",
    "                               flags=cv2.IMREAD_COLOR)\n",
    "for frame_index, frame in enumerate(frames):\n",
    "    # Convert the frame to RGB format if it's grayscale (required by PIL)\n",
    "#     if len(frame.shape) == 2:  # grayscale\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    for trajIdx, traj in trajDic.items():\n",
    "        fIdxS = traj[0, 0]  # traj starting frm\n",
    "        fIdxE = traj[-1, 0]  # traj starting frm\n",
    "        if fIdxS <= frame_index  <= fIdxE: \n",
    "            traj_point = traj[(traj == float(frame_index))[:, 0]]\n",
    "            cv2.putText(frame, trajIdx, (int(traj_point[0, 2]-7), int(traj_point[0, 3]-10)), font, fontScale, fontColor, thickness)\n",
    "            \n",
    "    frames_processed.append(Image.fromarray(frame))\n",
    "\n",
    "# Save the processed frames as a TIFF stack\n",
    "frames_processed[0].save(outpath, save_all=True, append_images=frames_processed[1:], compression=\"tiff_deflate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sample code for skeleton analysis\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.graph import route_through_array\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Load binary image (already binarized)\n",
    "binary = cv2.imread(\"banana_shape.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Create a color image to visualize results\n",
    "output_image = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "for i, ct in enumerate(contours):\n",
    "    if len(ct) < 5:\n",
    "        continue  # Need at least 5 points for ellipse fitting\n",
    "\n",
    "    # (1) Fit an ellipse\n",
    "    ellipse = cv2.fitEllipse(ct)  # (center_x, center_y), (major_axis, minor_axis), angle\n",
    "    cv2.ellipse(output_image, ellipse, (0, 255, 0), 2)  # Draw the ellipse in green\n",
    "\n",
    "    # Create an empty mask for the current contour\n",
    "    contour_mask = np.zeros_like(binary)\n",
    "    cv2.drawContours(contour_mask, [ct], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # (2) Skeletonize the filled contour\n",
    "    skeleton = skeletonize(contour_mask // 255)  # Convert 255 -> 1 before skeletonizing\n",
    "\n",
    "    # Get skeleton pixel coordinates\n",
    "    skel_coords = np.column_stack(np.where(skeleton > 0))  # Get (row, col) positions\n",
    "\n",
    "    # Compute Geodesic Distance (longest shortest path along the skeleton)\n",
    "    if len(skel_coords) > 1:\n",
    "        # Compute pairwise distances along the skeleton\n",
    "        dist_matrix = squareform(pdist(skel_coords))  \n",
    "        max_distance = 0\n",
    "        max_pair = None\n",
    "\n",
    "        # Find the two most distant points in the skeleton\n",
    "        for i in range(len(skel_coords)):\n",
    "            for j in range(i + 1, len(skel_coords)):\n",
    "                if dist_matrix[i, j] > max_distance:\n",
    "                    max_distance = dist_matrix[i, j]\n",
    "                    max_pair = (skel_coords[i], skel_coords[j])\n",
    "\n",
    "        if max_pair:\n",
    "            # Compute the geodesic distance using shortest path in skeleton\n",
    "            cost_array = np.ones_like(binary, dtype=np.float32)  # Uniform cost map\n",
    "            geodesic_distance, _ = route_through_array(\n",
    "                cost_array, max_pair[0], max_pair[1], fully_connected=True\n",
    "            )\n",
    "            geodesic_distance = np.sum(geodesic_distance)  # Total path length\n",
    "        else:\n",
    "            geodesic_distance = 0\n",
    "    else:\n",
    "        geodesic_distance = 0  # No valid skeleton\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Contour {i+1}:\")\n",
    "    print(f\"  Geodesic Distance: {geodesic_distance:.2f} pixels\")\n",
    "    print(f\"  Ellipse Center: {ellipse[0]}, Axes: {ellipse[1]}, Angle: {ellipse[2]}\\n\")\n",
    "\n",
    "# Display results\n",
    "cv2.imshow(\"Ellipses on Contours\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
